{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-19 12:15:32,126 - INFO: scraping https://dynamic2.scrape.center/page/1\n",
      "2021-06-19 12:15:34,780 - INFO: details urls ['https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIx', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIy', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIz', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI0', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI1', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI2', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI3', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI4', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWI5', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxMA==']\n",
      "2021-06-19 12:15:34,781 - INFO: scraping https://dynamic2.scrape.center/page/2\n",
      "2021-06-19 12:15:35,816 - INFO: details urls ['https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxMQ==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxMg==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxMw==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxNA==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxNQ==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxNg==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxNw==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxOA==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIxOQ==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyMA==']\n",
      "2021-06-19 12:15:35,817 - INFO: scraping https://dynamic2.scrape.center/page/3\n",
      "2021-06-19 12:15:37,279 - INFO: details urls ['https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyMQ==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyMg==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyMw==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyNA==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIzNg==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyNQ==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyNg==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyNw==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyOA==', 'https://dynamic2.scrape.center/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIyOQ==']\n",
      "2021-06-19 12:15:37,280 - INFO: scraping https://dynamic2.scrape.center/page/4\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import logging\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "\n",
    "INDEX_URL = 'https://dynamic2.scrape.center/page/{page}'\n",
    "TIME_OUT = 20\n",
    "TOTAL_PAGE = 10\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "wait = WebDriverWait(browser, TIME_OUT)\n",
    "\n",
    "def scrape_page(url, condition, locator):\n",
    "\tlogging.info('scraping %s', url)\n",
    "\ttry:\n",
    "\t\tbrowser.get(url)\n",
    "\t\twait.until(condition(locator))\n",
    "\texcept TimeoutException:\n",
    "\t\tlogging.error('error occurred while scraping %s', url, exc_info=True)\n",
    "\t\t\n",
    "def scrape_index(page):\n",
    "\turl = INDEX_URL.format(page=page)\n",
    "\tscrape_page(url, condition=EC.visibility_of_all_elements_located, locator=(By.CSS_SELECTOR, '.name'))\n",
    "\t\n",
    "def parse_index():\n",
    "\telements = browser.find_elements_by_css_selector('#index .item .name')\n",
    "\tfor element in elements:\n",
    "\t\thref = element.get_attribute('href')\n",
    "\t\tyield urljoin(INDEX_URL, href)\n",
    "\t\t\n",
    "def main():\n",
    "\ttry:\n",
    "\t\tfor page in range(1, TOTAL_PAGE + 1):\n",
    "\t\t\tscrape_index(page)\n",
    "\t\t\tdetail_urls = parse_index()\n",
    "\t\t\tlogging.info('details urls %s', list(detail_urls))\n",
    "\tfinally:\n",
    "\t\tbrowser.close()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import logging\n",
    "import json\n",
    "from motor.motor_asyncio import AsyncIOMotorClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import logging\n",
    "import json\n",
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "INDEX_URL = 'https://dynamic5.scrape.center/api/book/?limit=18&offset={offset}'\n",
    "DETAIL_URL = 'https://dynamic5.scrape.center/api/book/{id}'\n",
    "PAGE_SIZE = 18\n",
    "PAGE_NUMBER = 100\n",
    "CONCURRENCY = 5\n",
    "\n",
    "semaphore = asyncio.Semaphore(CONCURRENCY)\n",
    "session = None\n",
    "\n",
    "async def scrape_api(url):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            logging.info('scraping %s', url)\n",
    "            async with session.get(url) as response:\n",
    "                return await response.json()\n",
    "        except aiohttp.ClientError:\n",
    "            logging.error('error occured while scraping %s', url, exc_info=True)\n",
    "            \n",
    "async def scrape_index(page):\n",
    "    url = INDEX_URL.format(offset=PAGE_SIZE * (page - 1))\n",
    "    return await scrape_api(url)\n",
    "\n",
    "MONGO_CONNECTION_STRING = 'mongodb://localhost:27017'\n",
    "MONGO_DB_NAME = 'books'\n",
    "MONGO_COLLECTION_NAME = 'books'\n",
    "client = AsyncIOMotorClient(MONGO_CONNECTION_STRING)\n",
    "db = client[MONGO_DB_NAME]\n",
    "collection = db[MONGO_COLLECTION_NAME]\n",
    "\n",
    "async def save_data(data):\n",
    "    logging.info('saving data %s', data)\n",
    "    if data:\n",
    "        return await collection.update_one({'id': data.get('id)')}, {'$set': data}, upsert=True)\n",
    "    \n",
    "async def scrape_detail(id):\n",
    "    url = DETAIL_URL.format(id=id)\n",
    "    data = await scrape_api(url)\n",
    "    await save_data(data)\n",
    "\n",
    "async def main():\n",
    "    global session\n",
    "    ids = []\n",
    "    session = aiohttp.ClientSession()\n",
    "    scrape_index_tasks = [asyncio.ensure_future(scrape_index(page)) for page in range(1, PAGE_NUMBER + 1)]\n",
    "    results = await asyncio.gather(*scrape_index_tasks)\n",
    "#     logging.info('results %s', json.dumps(results, ensure_ascii=False, indent=2))\n",
    "    for index_data in results:\n",
    "        if not index_data:\n",
    "            continue\n",
    "        for item in index_data.get('results'):\n",
    "            ids.append(item.get('id'))\n",
    "            \n",
    "    scrape_detail_tasks = [asyncio.ensure_future(scrape_detail(id)) for id in ids]\n",
    "    await asyncio.wait(scrape_detail_tasks)\n",
    "    await session.close()\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    asyncio.get_event_loop().run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyppeteer.chromium_downloader\n",
    "print('默认版本是：{}'.format(pyppeteer.__chromium_revision__))\n",
    "print('安装路径是：{}'.format(pyppeteer.__pyppeteer_home__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "async def main():\n",
    "    browser = await launch()\n",
    "    page = await browser.newPage()\n",
    "    await page.goto('https://dynamic2.scrape.center/')\n",
    "    await page.waitForSelector('.item .name')\n",
    "    doc = pq(await page.content())\n",
    "    names = [item.text() for item in doc('.item .name').items()]\n",
    "    print('Names:', names)\n",
    "    await browser.close()\n",
    "asyncio.get_event_loop().run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "\n",
    "width , height = 1366, 768\n",
    "\n",
    "async def main():\n",
    "    browser = await launch(headless=False, userDataDir='./userdata', args=['--disable-infobars', f'--windows-size={width},{height}'])\n",
    "    page = await browser.newPage()\n",
    "    await page.goto('https://www.taobao.com')\n",
    "    await asyncio.sleep(100)\n",
    "    await browser.close()\n",
    "                                                                           \n",
    "asyncio.get_event_loop().run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "\n",
    "width , height = 1366, 768\n",
    "\n",
    "async def main():\n",
    "    browser = await launch(headless=False, userDataDir='./userdata', args=['--disable-infobars', f'--window-size={width},{height}'])\n",
    "    context = await browser.createIncognitoBrowserContext()\n",
    "    page = await context.newPage()\n",
    "    await page.setViewport({'width': width, 'height': height})\n",
    "    await page.goto('https://www.baidu.com')\n",
    "    await asyncio.sleep(30)\n",
    "    await browser.close()\n",
    "                                                                           \n",
    "asyncio.get_event_loop().run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "width , height = 1366, 768\n",
    "\n",
    "async def main():\n",
    "    browser = await launch(headless=False, args=['--disable-infobars', f'--window-size={width},{height}'])\n",
    "    page = await browser.newPage()\n",
    "    await page.setViewport({'width': width, 'height': height})\n",
    "    \n",
    "    await page.goto('https://dynamic2.scrape.center/')\n",
    "    await page.waitForSelector('.item .name')\n",
    "    \n",
    "    j_result1 = await page.J('.item .name')\n",
    "    j_result2 = await page.querySelector('.item .name')\n",
    "    jj_result1 = await page.JJ('.item .name')\n",
    "    jj_result2 = await page.querySelectorALL('.item .name')\n",
    "    \n",
    "    print('J Result1:', j_result1)\n",
    "    print('J Result2:', j_result2)\n",
    "    print('JJ Result1:', jj_result1)\n",
    "    print('JJ Result1:', jj_result2)\n",
    "    \n",
    "    await browser.close()\n",
    "    \n",
    "asyncio.get_event_loop().run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "from pyppeteer.errors import TimeoutError\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "\n",
    "INDEX_URL = 'https://dynamic2.scrape.center/page/{page}'\n",
    "TIME_OUT = 10\n",
    "TOTAL_PAGE = 10\n",
    "WINDOW_WIDTH, WINDOW_HEIGHT = 1366, 768\n",
    "HEADLESS = False\n",
    "\n",
    "browser, tab = None, None\n",
    "\n",
    "async def init():\n",
    "    global browser, tab\n",
    "    browser = await launch(headless=HEADLESS, args=['--disable-infobars', f'--window-size={WINDOW_WIDTH},{WINDOW_HEIGHT}'])\n",
    "    tab = await browser.newPage()\n",
    "    await tab.setViewport({'width': WINDOW_WIDTH, 'height': WINDOW_HEIGHT})\n",
    "\n",
    "async def scrape_page(url, selector):\n",
    "    logging.info('Scraping %s', url)\n",
    "    try:\n",
    "        await tab.goto(url)\n",
    "        await tab.waitForSelector(selector, options={\n",
    "            'timeout': TIME_OUT*1000\n",
    "        })\n",
    "    except TimeoutError:\n",
    "            logging.error('error occurred while scraping %s', url, exc_info=True)\n",
    "            \n",
    "async def scrape_index(page):\n",
    "    url = INDEX_URL.format(page=page)\n",
    "    await scrape_page(url, '.item .name')\n",
    "    \n",
    "async def parse_index():\n",
    "    return await tab.querySelectorAllEval('.item .name', 'nodes => nodes.map(node => node.href)')\n",
    "\n",
    "async def scrape_detail(url):\n",
    "    await scrape_page(url, 'h2')\n",
    "    \n",
    "async def parse_detail():\n",
    "    url = tab.url\n",
    "    name = await tab.querySelectorEval('h2', 'nodes => node.innerText')\n",
    "    categories = await tab.querySelectorAllEval('.categories button span', 'nodes => nodes.map(nodes => node.innerText)')\n",
    "    cover = await tab.querySelectorEval('.cover', 'nodes => node.src')\n",
    "    score = await tab.querySelectorEval('.score', 'nodes => node.innerText')\n",
    "    drama = await tab.querySelectorEval('.drama p', 'nodes => node.innerText')\n",
    "    return {\n",
    "        'url': url,\n",
    "        'name': name,\n",
    "        'categories': categories,\n",
    "        'cover': cover,\n",
    "        'score': score,\n",
    "        'drama': drama\n",
    "    }\n",
    "\n",
    "async def main():\n",
    "    await init()\n",
    "    try:\n",
    "        for page in range(1, TOTAL_PAGE + 1):\n",
    "            await scrape_index(page)\n",
    "            detail_urls = await parse_index()\n",
    "            for detail_url in detail_urls:\n",
    "                await scrape_detail(detail_url)\n",
    "                detail_data = parse_detail()\n",
    "                logging.info('data is : %s', detail_data)\n",
    "    finally:\n",
    "        await browser.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.get_event_loop().run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import asyncio\n",
    "from pyppeteer import launch\n",
    "from pyppeteer.errors import TimeoutError\n",
    "import json\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "\n",
    "INDEX_URL = 'https://dynamic2.scrape.center/page/{page}'\n",
    "TIME_OUT = 10\n",
    "TOTAL_PAGE = 10\n",
    "WINDOW_WIDTH, WINDOW_HEIGHT = 1366, 768\n",
    "HEADLESS = False\n",
    "\n",
    "browser, tab = None, None\n",
    "\n",
    "async def init():\n",
    "    global browser, tab\n",
    "    browser = await launch(headless=HEADLESS, args=['--disable-infobars', f'--window-size={WINDOW_WIDTH},{WINDOW_HEIGHT}'])\n",
    "    tab = await browser.newPage()\n",
    "    await tab.setViewport({'width': WINDOW_WIDTH, 'height': WINDOW_HEIGHT})\n",
    "\n",
    "async def scrape_page(url, selector):\n",
    "    logging.info('Scraping %s', url)\n",
    "    try:\n",
    "        await tab.goto(url)\n",
    "        await tab.waitForSelector(selector, options={\n",
    "            'timeout': TIME_OUT*1000\n",
    "        })\n",
    "    except TimeoutError:\n",
    "            logging.error('error occurred while scraping %s', url, exc_info=True)\n",
    "            \n",
    "async def scrape_index(page):\n",
    "    url = INDEX_URL.format(page=page)\n",
    "    await scrape_page(url, '.item .name')\n",
    "    \n",
    "async def parse_index():\n",
    "    return await tab.querySelectorAllEval('.item .name', 'nodes => nodes.map(node => node.href)')\n",
    "\n",
    "async def scrape_detail(url):\n",
    "    await scrape_page(url, 'h2')\n",
    "    \n",
    "async def parse_detail():\n",
    "    url = tab.url\n",
    "    name = await tab.querySelectorEval('h2', 'nodes => node.innerText')\n",
    "    categories = await tab.querySelectorAllEval('.categories button span', 'nodes => nodes.map(nodes => node.innerText)')\n",
    "    cover = await tab.querySelectorEval('.cover', 'nodes => node.src')\n",
    "    score = await tab.querySelectorEval('.score', 'nodes => node.innerText')\n",
    "    drama = await tab.querySelectorEval('.drama p', 'nodes => node.innerText')\n",
    "    return {\n",
    "        'url': url,\n",
    "        'name': name,\n",
    "        'categories': categories,\n",
    "        'cover': cover,\n",
    "        'score': score,\n",
    "        'drama': drama\n",
    "    }\n",
    "\n",
    "RESULTS_DIR = 'pyppeteer_results'\n",
    "exists(RESULTS_DIR) or makedirs(RESULTS_DIR)\n",
    "async def save_data(data):\n",
    "    name = date.get('name')\n",
    "    data_path = f'{RESULTS_DIR}/{name}.json'\n",
    "    json.dump(data, open(data_path, 'w', encoding='utf-8'), ensure_ascii=False, indent=2)\n",
    "\n",
    "async def main():\n",
    "    await init()\n",
    "    try:\n",
    "        for page in range(1, TOTAL_PAGE + 1):\n",
    "            await scrape_index(page)\n",
    "            detail_urls = await parse_index()\n",
    "            for detail_url in detail_urls:\n",
    "                await scrape_detail(detail_url)\n",
    "                detail_data = await parse_detail()\n",
    "                logging.info('data is : %s', detail_data)\n",
    "                await save_data(detail_data)\n",
    "    finally:\n",
    "        await browser.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.get_event_loop().run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
